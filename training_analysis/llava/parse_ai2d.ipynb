{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accuracy Evaluation Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import random\n",
    "\n",
    "random.seed(1234)\n",
    "\n",
    "def extract_option(text):\n",
    "    \"\"\"\n",
    "    Extract the option identifier from the end of a text.\n",
    "    \n",
    "    Args:\n",
    "        text (str): The input text to parse\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (full_match, option_value) where option_value is just the number/letter\n",
    "    \"\"\"\n",
    "    # Clean the text by removing any trailing whitespace\n",
    "    cleaned_text = text.strip()\n",
    "\n",
    "    # Punctuation stripping\n",
    "    for char in [\",\", \".\", \"!\", \"?\", \";\", \":\", \"'\"]:\n",
    "        cleaned_text = cleaned_text.strip(char)\n",
    "    \n",
    "    # Define patterns to search for options anywhere in the text\n",
    "    patterns = [\n",
    "        r'[Oo]ption\\s*(\\d+)',     # Option3, Option 3\n",
    "        r'[Oo]ption\\s*([A-Za-z])' # OptionA, Option A\n",
    "    ]\n",
    "    \n",
    "    # Try each pattern\n",
    "    for pattern in patterns:\n",
    "        matches = re.findall(pattern, cleaned_text)\n",
    "        if matches:\n",
    "            # Return the first option found\n",
    "            return answer_to_index(matches[0])\n",
    "    \n",
    "    # Look for standalone numbers or letters that might be options\n",
    "    # This is a fallback and may produce false positives\n",
    "    standalone_pattern = r'\\b([A-Za-z]|\\d+)\\b'\n",
    "    matches = re.findall(standalone_pattern, cleaned_text)\n",
    "    if matches:\n",
    "        # Try to find what looks most like an option (preference for single letters or numbers)\n",
    "        for match in matches:\n",
    "            if len(match) == 1 and match.isalpha():  # Single letter\n",
    "                return answer_to_index(match)\n",
    "            elif match.isdigit() and 1 <= int(match) <= 10:  # Reasonable option number\n",
    "                return answer_to_index(match)\n",
    "    \n",
    "    return answer_to_index(None)\n",
    "\n",
    "def answer_to_index(response):\n",
    "    \"\"\"\n",
    "    Return the predicted index of the parsed response, E.g., 0, 1, 2, 3\n",
    "    \"\"\"\n",
    "    try:\n",
    "        letter_choices = [\"a\", \"b\", \"c\", \"d\"]\n",
    "        if response.lower() in letter_choices:\n",
    "            response = letter_choices.index(response.lower())\n",
    "        \n",
    "        if int(response) > 3:\n",
    "            response = random.randint(0, 3)\n",
    "\n",
    "        return response\n",
    "    except Exception:\n",
    "        return random.randint(0, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing different formats:\n",
      "Text: So, the answer is: Option3.\n",
      "Index: 3\n",
      "\n",
      "Text: Therefore, option2 is correct.\n",
      "Index: 2\n",
      "\n",
      "Text: The correct answer is Option C.\n",
      "Index: 2\n",
      "\n",
      "Text: I believe option 6 is the answer.\n",
      "Index: 3\n",
      "\n",
      "Text: Hence, the solution is option A.\n",
      "Index: 0\n",
      "\n",
      "Text: The result is 5.\n",
      "Index: 0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test with various samples\n",
    "test_samples = [\n",
    "    \"So, the answer is: Option3.\",\n",
    "    \"Therefore, option2 is correct.\",\n",
    "    \"The correct answer is Option C.\",\n",
    "    \"I believe option 6 is the answer.\",\n",
    "    \"Hence, the solution is option A.\",\n",
    "    \"The result is 5.\"\n",
    "]\n",
    "\n",
    "print(\"Testing different formats:\")\n",
    "for sample in test_samples:\n",
    "    index = extract_option(sample)\n",
    "    print(f\"Text: {sample}\")\n",
    "    print(f\"Index: {index}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "\n",
    "def eval(response_files, out_files):\n",
    "    \"\"\"\n",
    "    Evaluate the accuracy of the model.\n",
    "    \"\"\"\n",
    "    all_accs = []\n",
    "    for response, out in zip(response_files, out_files):\n",
    "        data = [json.loads(line) for line in open(response)]\n",
    "        out_file = open(out, \"w\")\n",
    "\n",
    "        accs = []\n",
    "        correct = []\n",
    "        incorrect = []\n",
    "        for item in data:\n",
    "            prompt = item[\"prompt\"]\n",
    "            label = item[\"question_id\"]\n",
    "            gen = item[\"text\"]\n",
    "\n",
    "            pred = extract_option(gen)\n",
    "\n",
    "            acc = pred == label\n",
    "            accs.append(acc)\n",
    "\n",
    "            if acc:\n",
    "                correct.append((prompt, gen, label))\n",
    "            else:\n",
    "                incorrect.append((prompt, gen, label))\n",
    "\n",
    "        all_accs.append(accs)\n",
    "        print(response, len(accs), np.mean(accs), file=out_file) # Send output to file\n",
    "\n",
    "        # Print correct and incorrect predictions too\n",
    "        print(\"Correct predictions:\", file=out_file)\n",
    "\n",
    "        for pred in correct:\n",
    "            print(f\"Prompt:\", pred[0], file=out_file)\n",
    "            print(f\"Model output:\", pred[1], file=out_file)\n",
    "            print(f\"Actual answer:\", pred[2], file=out_file)\n",
    "            print(\"\", file=out_file)\n",
    "\n",
    "        print(\"\\n-------------------------------------------\", file=out_file)\n",
    "        print(\"Incorrect predictions:\", file=out_file)\n",
    "        for pred in incorrect:\n",
    "            print(f\"Prompt:\", pred[0], file=out_file)\n",
    "            print(f\"Model output:\", pred[1], file=out_file)\n",
    "            print(f\"Actual answer:\", pred[2], file=out_file)\n",
    "            print(\"\", file=out_file)\n",
    "            \n",
    "    return all_accs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_files = [\"./playground/data/ai2d_predictions_llava-7b_imagenet-and-llava-trained.jsonl\", \"./playground/data/ai2d_predictions_llava-7b_base.jsonl\"]\n",
    "out_files = [\"results/eval_ai2d_combined.txt\", \"results/eval_ai2d_base.txt\"]\n",
    "all_accs = eval(response_files, out_files)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llava",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
